{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b2025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hassa\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ebb659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\agent.py:5: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agent import Agent\n",
    "from dataset import Dataset\n",
    "from network import QNetwork\n",
    "from memory import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad82bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassavaLeafDataset:\n",
    "    def __init__(self, image_size = (512, 512), batch_size=32):\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.reading_csv(\"../Deep-Reinforcement-Learning-on-Imbalanced-Data/cassava-leaf-disease-classification/train_images/\", \"../Deep-Reinforcement-Learning-on-Imbalanced-Data/cassava-leaf-disease-classification/train.csv\")\n",
    "        self.create_dataset()\n",
    "\n",
    "        self.get_rho()\n",
    "        self.get_minority_classes()\n",
    "\n",
    "    def reading_csv(self, folder_path, file_path):\n",
    "        df = pd.read_csv(file_path) # Load train image file names and each label data\n",
    "        df[\"filepath\"] = folder_path + df[\"image_id\"] # Create path by adding folder name and image name for load images easily\n",
    "        df = df.drop(['image_id'],axis=1) # Drop image names which is useless.\n",
    "        self.X = df.drop(columns=[\"label\"])\n",
    "        self.y = df['label']\n",
    "#         self.y = tf.keras.utils.to_categorical(df[\"label\"].values)\n",
    "\n",
    "    \n",
    "    def imbalance_the_data(self):\n",
    "        pass\n",
    "\n",
    "    def load_image_and_label_from_path(self, image_path, label):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, self.image_size)\n",
    "        return img, label\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, random_state=42, test_size=0.2)\n",
    "        \n",
    "        training_data = tf.data.Dataset.from_tensor_slices((X_train.filepath.values, y_train))\n",
    "        testing_data = tf.data.Dataset.from_tensor_slices((X_test.filepath.values, y_test))\n",
    "\n",
    "        AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "        training_data = training_data.map(self.load_image_and_label_from_path, num_parallel_calls=AUTOTUNE)\n",
    "        testing_data = testing_data.map(self.load_image_and_label_from_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "        self.training_data_batches = training_data.shuffle(buffer_size=1000).batch(self.batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "        self.testing_data_batches = testing_data.shuffle(buffer_size=1000).batch(self.batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "        \n",
    "        self.x_train = []\n",
    "        self.y_train = []\n",
    "        self.x_test = []\n",
    "        self.y_test = []\n",
    "        \n",
    "        # Create a TensorFlow session\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            train_iterator = tf.compat.v1.data.make_one_shot_iterator(self.training_data_batches)\n",
    "            train_next_element = train_iterator.get_next()\n",
    "        \n",
    "            while True:\n",
    "                try:\n",
    "                    features, labels = sess.run(train_next_element)\n",
    "                    for i in range(len(labels)):\n",
    "                        self.x_train.append(features[i])\n",
    "                        self.y_train.append((labels[i],))    \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "            \n",
    "            test_iterator = tf.compat.v1.data.make_one_shot_iterator(self.testing_data_batches)\n",
    "            test_next_element = test_iterator.get_next()\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    features, labels = sess.run(test_next_element)\n",
    "                    for i in range(len(labels)):\n",
    "                        self.x_test.append(features[i])\n",
    "                        self.y_test.append((labels[i],))\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "        print(len(self.y_train))\n",
    "        print(len(self.y_test))\n",
    "        return self.training_data_batches, self.testing_data_batches\n",
    "\n",
    "    def get_class_num(self):\n",
    "        # get number of all classes\n",
    "#         _, nums_cls = np.unique(np.argmax(self.y, axis=1), return_counts=True)\n",
    "        _, nums_cls = np.unique(self.y, return_counts=True)\n",
    "        print(\"No of total samples in dataset and their distribution: \", np.unique(self.y, return_counts=True))\n",
    "        \n",
    "        return nums_cls\n",
    "\n",
    "    def get_minority_classes(self):\n",
    "        label, label_count = np.unique(self.y, return_counts=True)\n",
    "        print(\"Label is \", label)\n",
    "        labels_with_counts = {}\n",
    "        for i in range(len(label)):\n",
    "            labels_with_counts[label[i]] = label_count[i]\n",
    "        print(\"Labels with count\",labels_with_counts)\n",
    "        labels_with_counts = sorted(labels_with_counts.items())\n",
    "\n",
    "        # We are going to get 35% minority classes from total classes i-e if there are total 6 classes then we will only set 2 classes as minority classes\n",
    "        no_of_minority_classes_to_get = int(np.round(len(label) * 0.35))\n",
    "\n",
    "        self.minority_classes = []\n",
    "        for i in range(no_of_minority_classes_to_get):\n",
    "            self.minority_classes.append(labels_with_counts[i][0])\n",
    "\n",
    "    def get_rho(self):\n",
    "        \"\"\"\n",
    "        In the two-class dataset problem, research paper has proven that the best performance is achieved when the reciprocal of the ratio of the number of data is used as the reward function.\n",
    "        In this code, the result of this paper is extended to multi-class by creating a reward function with the reciprocal of the number of data for each class.\n",
    "        \"\"\"\n",
    "        nums_cls = self.get_class_num()\n",
    "        raw_reward_set = 1 / nums_cls\n",
    "        self.reward_set = np.round(raw_reward_set / np.linalg.norm(raw_reward_set), 6)\n",
    "        print(\"\\nReward for each class.\")\n",
    "        for cl_idx, cl_reward in enumerate(self.reward_set):\n",
    "            print(\"\\t- Class {} : {:.6f}\".format(cl_idx, cl_reward))\n",
    "\n",
    "\n",
    "class PersonalityDataset:\n",
    "    def __init__(self, batch_size=100):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.create_dataset()\n",
    "        self.get_rho()\n",
    "        self.get_minority_classes()\n",
    "\n",
    "    def create_dataset(self):\n",
    "        df = pd.read_csv(\"../16P/16P.csv\", encoding='cp1252')\n",
    "        \n",
    "        df = df.dropna()\n",
    "\n",
    "        self.X = df.drop([\"Personality\", \"Response Id\"], axis = 1)\n",
    "        self.y = df[\"Personality\"]\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.y = self.label_encoder.fit_transform(self.y)\n",
    "        \n",
    "        self.y = tf.keras.utils.to_categorical(self.y)\n",
    "\n",
    "        self.unique_labels, self.label_counts = np.unique((np.argmax(self.y, axis=1)), return_counts=True)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X.values, self.y, random_state=42, test_size=0.2)\n",
    "        \n",
    "        # We are going to get 25% minority classes from total classes i-e if there are total 6 classes then we will only set 2 classes as minority classes\n",
    "        self.no_of_minority_classes_to_get = int(np.round(len(np.unique(np.argmax(y_train, axis=1))) * 0.25))\n",
    "        \n",
    "        # Specify the percentage of label 2 data to remove\n",
    "        percentage_to_remove = 90\n",
    "        for class_to_remove in range(self.no_of_minority_classes_to_get):\n",
    "            argmax_values = np.argmax(y_train, axis=1)\n",
    "            indices_to_remove = np.unique(np.where(argmax_values == class_to_remove)[0])\n",
    "            # Calculate the number of samples to remove\n",
    "            num_samples_to_remove = int(percentage_to_remove / 100 * len(indices_to_remove))\n",
    "\n",
    "            # Randomly select indices to remove\n",
    "            indices_to_remove = np.random.choice(indices_to_remove, num_samples_to_remove, replace=False)\n",
    "            # Remove the selected samples\n",
    "            percentage_to_remove -= 10\n",
    "\n",
    "            # Remove the selected samples\n",
    "            X_train = np.delete(X_train, indices_to_remove, axis=0)\n",
    "            y_train = np.delete(y_train, indices_to_remove, axis=0)\n",
    "            percentage_to_remove -= 10\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "        self.length_of_dataset = len(X_train)\n",
    "        \n",
    "    def get_labels_counts(self):\n",
    "        self.unique_labels, self.label_counts = np.unique(np.argmax(self.y_train, axis=1), return_counts=True)\n",
    "        \n",
    "        return self.label_counts\n",
    "        \n",
    "    def get_minority_classes(self):\n",
    "        \n",
    "        unique_labels, label_counts = np.unique((np.argmax(self.y_train, axis=1)), return_counts=True)\n",
    "        print(\"Label is \", unique_labels)\n",
    "        labels_with_counts = {}\n",
    "\n",
    "\n",
    "        for i in range(len(unique_labels)):\n",
    "            labels_with_counts[unique_labels[i]] = label_counts[i]\n",
    "        \n",
    "        print(\"Labels with count\",labels_with_counts)\n",
    "        labels_with_counts = sorted(labels_with_counts.items())\n",
    "\n",
    "        # We are going to get 35% minority classes from total classes i-e if there are total 6 classes then we will only set 2 classes as minority classes\n",
    "        no_of_minority_classes_to_get = int(np.round(len(unique_labels) * 0.35))\n",
    "\n",
    "        self.minority_classes = []\n",
    "        for i in range(no_of_minority_classes_to_get):\n",
    "            self.minority_classes.append(labels_with_counts[i][0])\n",
    "\n",
    "    def get_rho(self):\n",
    "        \"\"\"\n",
    "        In the two-class dataset problem, research paper has proven that the best performance is achieved when the reciprocal of the ratio of the number of data is used as the reward function.\n",
    "        In this code, the result of this paper is extended to multi-class by creating a reward function with the reciprocal of the number of data for each class.\n",
    "        \"\"\"\n",
    "        nums_cls = self.get_labels_counts()\n",
    "        raw_reward_set = 1 / nums_cls\n",
    "        self.reward_set = np.round(raw_reward_set / np.linalg.norm(raw_reward_set), 6)\n",
    "        print(\"\\nReward for each class.\")\n",
    "        for cl_idx, cl_reward in enumerate(self.reward_set):\n",
    "            print(\"\\t- Class {} : {:.6f}\".format(cl_idx, cl_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21a9cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, new_class, minor_classes, train_step, restore_model_path, gamma, learning_rate, batch, \n",
    "                epsilon_range, epsilon_polynomial_decay_step, target_soft_update, target_update_step, save_term, evaluation_term,\n",
    "                show_phase):\n",
    "        self.new_class = new_class\n",
    "        self.minor_classes = minor_classes\n",
    "        self.train_step = train_step\n",
    "        self.restore_model_path = restore_model_path\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch = batch\n",
    "        self.epsilon_range = epsilon_range\n",
    "        self.epsilon_polynomial_decay_step = epsilon_polynomial_decay_step\n",
    "        self.target_soft_update = target_soft_update\n",
    "        self.target_update_step = target_update_step\n",
    "        self.save_folder = '.model'\n",
    "        self.save_term = save_term\n",
    "        self.evaluation_term = evaluation_term\n",
    "        self.show_phase = show_phase\n",
    "\n",
    "config = Config({0:[0], 1:[1], 2:[2], 3: [3], 4:[4]}, [0, 1], 120000, '', 0.1, 0.001, 32, [0.01, 1], 120000, 1., 1000, 120000, 1000, 'Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3e70824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(config.new_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c635da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9d281a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17117\n",
      "4280\n",
      "17117\n",
      "4280\n",
      "No of total samples in dataset and their distribution:  (array([0, 1, 2, 3, 4], dtype=int64), array([ 1087,  2189,  2386, 13158,  2577], dtype=int64))\n",
      "\n",
      "Reward for each class.\n",
      "\t- Class 0 : 0.781136\n",
      "\t- Class 1 : 0.387891\n",
      "\t- Class 2 : 0.355865\n",
      "\t- Class 3 : 0.064531\n",
      "\t- Class 4 : 0.329489\n",
      "Label is  [0 1 2 3 4]\n",
      "Labels with count {0: 1087, 1: 2189, 2: 2386, 3: 13158, 4: 2577}\n"
     ]
    }
   ],
   "source": [
    "dataset = CassavaLeafDataset(image_size = (128, 128), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e22c3ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22a08728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:63: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  x = tf.compat.v1.layers.conv2d(self.state, 64, 5, strides=2, activation=tf.nn.relu)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:64: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  x = tf.compat.v1.layers.conv2d(x, 32, 5, strides=2, activation=tf.nn.relu)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:65: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
      "  x = tf.compat.v1.layers.max_pooling2d(x, (2,2), strides=2)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:67: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  x = tf.compat.v1.layers.conv2d(x, 64, 5, strides=1, activation=tf.nn.relu)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:68: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  x = tf.compat.v1.layers.conv2d(x, 32, 5, strides=1, activation=tf.nn.relu)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:69: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
      "  x = tf.compat.v1.layers.max_pooling2d(x, (2,2), strides=2)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:71: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  x = tf.compat.v1.layers.flatten(x)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:72: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  x = tf.compat.v1.layers.dense(x, 256, activation=tf.nn.relu)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:73: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  x = tf.compat.v1.layers.dense(x, 128, activation=tf.nn.relu)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:74: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  a = tf.compat.v1.layers.dense(x, self.n_class)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:75: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  v = tf.compat.v1.layers.dense(x, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "train_step : 1000, epsilon : 0.992\n",
      "Accuracy Score:  0.10163551401869159\n",
      "\t\t Validation Data.     f1-score of class 0 : 0.000, class 1 : 0.185, class 2 : 0.000, class 3 : 0.000, class 4 : 0.000, weighted macro avg : 0.019\n",
      "train_step : 2000, epsilon : 0.984\n",
      "Accuracy Score:  0.10163551401869159\n",
      "\t\t Validation Data.     f1-score of class 0 : 0.000, class 1 : 0.185, class 2 : 0.000, class 3 : 0.000, class 4 : 0.000, weighted macro avg : 0.019\n",
      "train_step : 3000, epsilon : 0.975\n",
      "Accuracy Score:  0.624766355140187\n",
      "\t\t Validation Data.     f1-score of class 0 : 0.000, class 1 : 0.000, class 2 : 0.000, class 3 : 0.769, class 4 : 0.000, weighted macro avg : 0.480\n",
      "train_step : 4000, epsilon : 0.967\n",
      "Accuracy Score:  0.624766355140187\n",
      "\t\t Validation Data.     f1-score of class 0 : 0.000, class 1 : 0.000, class 2 : 0.000, class 3 : 0.769, class 4 : 0.000, weighted macro avg : 0.480\n",
      "train_step : 5000, epsilon : 0.959\n",
      "Accuracy Score:  0.11612149532710281\n",
      "\t\t Validation Data.     f1-score of class 0 : 0.000, class 1 : 0.000, class 2 : 0.000, class 3 : 0.000, class 4 : 0.208, weighted macro avg : 0.024\n",
      "train_step : 6000, epsilon : 0.951\n",
      "Accuracy Score:  0.10163551401869159\n",
      "\t\t Validation Data.     f1-score of class 0 : 0.000, class 1 : 0.185, class 2 : 0.000, class 3 : 0.000, class 4 : 0.000, weighted macro avg : 0.019\n",
      "train_step : 7000, epsilon : 0.942\n",
      "Accuracy Score:  0.04906542056074766\n",
      "\t\t Validation Data.     f1-score of class 0 : 0.094, class 1 : 0.000, class 2 : 0.000, class 3 : 0.000, class 4 : 0.000, weighted macro avg : 0.005\n",
      "train_step : 8000, epsilon : 0.934\n",
      "Accuracy Score:  0.10163551401869159\n",
      "\t\t Validation Data.     f1-score of class 0 : 0.000, class 1 : 0.185, class 2 : 0.000, class 3 : 0.000, class 4 : 0.000, weighted macro avg : 0.019\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m memory \u001b[38;5;241m=\u001b[39m Memory()\n\u001b[0;32m      3\u001b[0m agent \u001b[38;5;241m=\u001b[39m Agent(q_network, dataset, memory, config)\n\u001b[1;32m----> 4\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32m~\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\agent.py:82\u001b[0m, in \u001b[0;36mAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m a_wrt_qmnet \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(q_mnet, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[:, np\u001b[38;5;241m.\u001b[39mnewaxis]  \u001b[38;5;66;03m# (batch, 1)\u001b[39;00m\n\u001b[0;32m     81\u001b[0m max_q_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtake_along_axis(q_tnet, a_wrt_qmnet, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (batch, 1)\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msess\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mtrain_op, feed_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mstate: sample_s, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39maction: sample_a,\n\u001b[0;32m     83\u001b[0m                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mreward: sample_r, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mterminal: sample_t,\n\u001b[0;32m     84\u001b[0m                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mtarget_q: max_q_,\n\u001b[0;32m     85\u001b[0m                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mlearning_rate: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlearning_rate})\n\u001b[0;32m     86\u001b[0m pred_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m*\u001b[39ma)\n\u001b[0;32m     87\u001b[0m label_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m*\u001b[39my)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:972\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    969\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 972\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\u001b[38;5;28;01mNone\u001b[39;00m, fetches, feed_dict, options_ptr,\n\u001b[0;32m    973\u001b[0m                      run_metadata_ptr)\n\u001b[0;32m    974\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    975\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1215\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1215\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_run(handle, final_targets, final_fetches,\n\u001b[0;32m   1216\u001b[0m                          feed_dict_tensor, options, run_metadata)\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1218\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1395\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1392\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1395\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m   1396\u001b[0m                        run_metadata)\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1398\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1402\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1401\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1403\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1404\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1385\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1383\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1384\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1385\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m   1386\u001b[0m                                   target_list, run_metadata)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1478\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1476\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1477\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1478\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf_session\u001b[38;5;241m.\u001b[39mTF_SessionRun_wrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session, options, feed_dict,\n\u001b[0;32m   1479\u001b[0m                                           fetch_list, target_list,\n\u001b[0;32m   1480\u001b[0m                                           run_metadata)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "q_network = QNetwork(config, 128, 'complex')\n",
    "memory = Memory()\n",
    "agent = Agent(q_network, dataset, memory, config)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983087f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
