{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e875cb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hassa\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ed53e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\agent.py:5: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from agent import Agent\n",
    "from dataset import Dataset\n",
    "from network import QNetwork\n",
    "from memory import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c89136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CassavaLeafDataset:\n",
    "    def __init__(self, image_size = (512, 512), batch_size=32):\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.reading_csv(\"../Deep-Reinforcement-Learning-on-Imbalanced-Data/cassava-leaf-disease-classification/train_images/\", \"../Deep-Reinforcement-Learning-on-Imbalanced-Data/cassava-leaf-disease-classification/train.csv\")\n",
    "        self.create_dataset()\n",
    "\n",
    "        self.get_rho()\n",
    "        self.get_minority_classes()\n",
    "\n",
    "    def reading_csv(self, folder_path, file_path):\n",
    "        df = pd.read_csv(file_path) # Load train image file names and each label data\n",
    "        df[\"filepath\"] = folder_path + df[\"image_id\"] # Create path by adding folder name and image name for load images easily\n",
    "        df = df.drop(['image_id'],axis=1) # Drop image names which is useless.\n",
    "        self.X = df.drop(columns=[\"label\"])\n",
    "        self.y = df['label']\n",
    "#         self.y = tf.keras.utils.to_categorical(df[\"label\"].values)\n",
    "\n",
    "    \n",
    "    def imbalance_the_data(self):\n",
    "        pass\n",
    "\n",
    "    def load_image_and_label_from_path(self, image_path, label):\n",
    "        img = tf.io.read_file(image_path)\n",
    "        img = tf.image.decode_jpeg(img, channels=3)\n",
    "        img = tf.image.resize(img, self.image_size)\n",
    "        return img, label\n",
    "    \n",
    "    def create_dataset(self):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, random_state=42, test_size=0.2)\n",
    "        \n",
    "        print(len(y_train))\n",
    "        print(len(y_test))\n",
    "        \n",
    "        training_data = tf.data.Dataset.from_tensor_slices((X_train.filepath.values, y_train))\n",
    "        testing_data = tf.data.Dataset.from_tensor_slices((X_test.filepath.values, y_test))\n",
    "\n",
    "        AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "        training_data = training_data.map(self.load_image_and_label_from_path, num_parallel_calls=AUTOTUNE)\n",
    "        testing_data = testing_data.map(self.load_image_and_label_from_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "        self.training_data_batches = training_data.shuffle(buffer_size=1000).batch(self.batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "        self.testing_data_batches = testing_data.shuffle(buffer_size=1000).batch(self.batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "        \n",
    "#         self.total_labels = len(np.unique(self.y))\n",
    "        self.x_train = []\n",
    "        self.y_train = []\n",
    "        self.x_test = []\n",
    "        self.y_test = []\n",
    "        \n",
    "        # Create a TensorFlow session\n",
    "        with tf.compat.v1.Session() as sess:\n",
    "            train_iterator = tf.compat.v1.data.make_one_shot_iterator(self.training_data_batches)\n",
    "            train_next_element = train_iterator.get_next()\n",
    "        \n",
    "            while True:\n",
    "                try:\n",
    "                    features, labels = sess.run(train_next_element)\n",
    "                    for i in range(len(labels)):\n",
    "                        self.x_train.append(features[i])\n",
    "                        self.y_train.append((labels[i],))    \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "            \n",
    "            test_iterator = tf.compat.v1.data.make_one_shot_iterator(self.testing_data_batches)\n",
    "            test_next_element = test_iterator.get_next()\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    features, labels = sess.run(test_next_element)\n",
    "                    for i in range(len(labels)):\n",
    "                        self.x_test.append(features[i])\n",
    "                        self.y_test.append((labels[i],))\n",
    "                        \n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    break\n",
    "        print(len(self.y_train))\n",
    "        print(len(self.y_test))\n",
    "        return self.training_data_batches, self.testing_data_batches\n",
    "\n",
    "    def get_class_num(self):\n",
    "        # get number of all classes\n",
    "#         _, nums_cls = np.unique(np.argmax(self.y, axis=1), return_counts=True)\n",
    "        _, nums_cls = np.unique(self.y, return_counts=True)\n",
    "        print(\"No of total samples in dataset and their distribution: \", np.unique(self.y, return_counts=True))\n",
    "        \n",
    "        return nums_cls\n",
    "\n",
    "    def get_minority_classes(self):\n",
    "        label, label_count = np.unique(self.y, return_counts=True)\n",
    "        print(\"Label is \", label)\n",
    "        labels_with_counts = {}\n",
    "        for i in range(len(label)):\n",
    "            labels_with_counts[label[i]] = label_count[i]\n",
    "        print(\"Labels with count\",labels_with_counts)\n",
    "        labels_with_counts = sorted(labels_with_counts.items())\n",
    "\n",
    "        # We are going to get 35% minority classes from total classes i-e if there are total 6 classes then we will only set 2 classes as minority classes\n",
    "        no_of_minority_classes_to_get = int(np.round(len(label) * 0.35))\n",
    "\n",
    "        self.minority_classes = []\n",
    "        for i in range(no_of_minority_classes_to_get):\n",
    "            self.minority_classes.append(labels_with_counts[i][0])\n",
    "\n",
    "    def get_rho(self):\n",
    "        \"\"\"\n",
    "        In the two-class dataset problem, research paper has proven that the best performance is achieved when the reciprocal of the ratio of the number of data is used as the reward function.\n",
    "        In this code, the result of this paper is extended to multi-class by creating a reward function with the reciprocal of the number of data for each class.\n",
    "        \"\"\"\n",
    "        nums_cls = self.get_class_num()\n",
    "        raw_reward_set = 1 / nums_cls\n",
    "        self.reward_set = np.round(raw_reward_set / np.linalg.norm(raw_reward_set), 6)\n",
    "        print(\"\\nReward for each class.\")\n",
    "        for cl_idx, cl_reward in enumerate(self.reward_set):\n",
    "            print(\"\\t- Class {} : {:.6f}\".format(cl_idx, cl_reward))\n",
    "\n",
    "\n",
    "class PersonalityDataset:\n",
    "    def __init__(self, batch_size=100):\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.create_dataset()\n",
    "        self.get_rho()\n",
    "        self.get_minority_classes()\n",
    "\n",
    "    def create_dataset(self):\n",
    "        df = pd.read_csv(\"../16P/16P.csv\", encoding='cp1252')\n",
    "        \n",
    "        df = df.dropna()\n",
    "\n",
    "        self.X = df.drop([\"Personality\", \"Response Id\"], axis = 1)\n",
    "        self.y = df[\"Personality\"]\n",
    "\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.y = self.label_encoder.fit_transform(self.y)\n",
    "        \n",
    "        self.y = tf.keras.utils.to_categorical(self.y)\n",
    "\n",
    "        self.unique_labels, self.label_counts = np.unique((np.argmax(self.y, axis=1)), return_counts=True)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.X.values, self.y, random_state=42, test_size=0.2)\n",
    "        \n",
    "        # We are going to get 25% minority classes from total classes i-e if there are total 6 classes then we will only set 2 classes as minority classes\n",
    "        self.no_of_minority_classes_to_get = int(np.round(len(np.unique(np.argmax(y_train, axis=1))) * 0.25))\n",
    "        \n",
    "        # Specify the percentage of label 2 data to remove\n",
    "        percentage_to_remove = 90\n",
    "        for class_to_remove in range(self.no_of_minority_classes_to_get):\n",
    "            argmax_values = np.argmax(y_train, axis=1)\n",
    "            indices_to_remove = np.unique(np.where(argmax_values == class_to_remove)[0])\n",
    "            # Calculate the number of samples to remove\n",
    "            num_samples_to_remove = int(percentage_to_remove / 100 * len(indices_to_remove))\n",
    "\n",
    "            # Randomly select indices to remove\n",
    "            indices_to_remove = np.random.choice(indices_to_remove, num_samples_to_remove, replace=False)\n",
    "            # Remove the selected samples\n",
    "            percentage_to_remove -= 10\n",
    "\n",
    "            # Remove the selected samples\n",
    "            X_train = np.delete(X_train, indices_to_remove, axis=0)\n",
    "            y_train = np.delete(y_train, indices_to_remove, axis=0)\n",
    "            percentage_to_remove -= 10\n",
    "\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "\n",
    "        self.length_of_dataset = len(X_train)\n",
    "        \n",
    "    def get_labels_counts(self):\n",
    "        self.unique_labels, self.label_counts = np.unique(np.argmax(self.y_train, axis=1), return_counts=True)\n",
    "        \n",
    "        return self.label_counts\n",
    "        \n",
    "    def get_minority_classes(self):\n",
    "        \n",
    "        unique_labels, label_counts = np.unique((np.argmax(self.y_train, axis=1)), return_counts=True)\n",
    "        print(\"Label is \", unique_labels)\n",
    "        labels_with_counts = {}\n",
    "\n",
    "\n",
    "        for i in range(len(unique_labels)):\n",
    "            labels_with_counts[unique_labels[i]] = label_counts[i]\n",
    "        \n",
    "        print(\"Labels with count\",labels_with_counts)\n",
    "        labels_with_counts = sorted(labels_with_counts.items())\n",
    "\n",
    "        # We are going to get 35% minority classes from total classes i-e if there are total 6 classes then we will only set 2 classes as minority classes\n",
    "        no_of_minority_classes_to_get = int(np.round(len(unique_labels) * 0.35))\n",
    "\n",
    "        self.minority_classes = []\n",
    "        for i in range(no_of_minority_classes_to_get):\n",
    "            self.minority_classes.append(labels_with_counts[i][0])\n",
    "\n",
    "    def get_rho(self):\n",
    "        \"\"\"\n",
    "        In the two-class dataset problem, research paper has proven that the best performance is achieved when the reciprocal of the ratio of the number of data is used as the reward function.\n",
    "        In this code, the result of this paper is extended to multi-class by creating a reward function with the reciprocal of the number of data for each class.\n",
    "        \"\"\"\n",
    "        nums_cls = self.get_labels_counts()\n",
    "        raw_reward_set = 1 / nums_cls\n",
    "        self.reward_set = np.round(raw_reward_set / np.linalg.norm(raw_reward_set), 6)\n",
    "        print(\"\\nReward for each class.\")\n",
    "        for cl_idx, cl_reward in enumerate(self.reward_set):\n",
    "            print(\"\\t- Class {} : {:.6f}\".format(cl_idx, cl_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c9ec748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, new_class, minor_classes, train_step, restore_model_path, gamma, learning_rate, batch, \n",
    "                epsilon_range, epsilon_polynomial_decay_step, target_soft_update, target_update_step, save_term, evaluation_term,\n",
    "                show_phase):\n",
    "        self.new_class = new_class\n",
    "        self.minor_classes = minor_classes\n",
    "        self.train_step = train_step\n",
    "        self.restore_model_path = restore_model_path\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch = batch\n",
    "        self.epsilon_range = epsilon_range\n",
    "        self.epsilon_polynomial_decay_step = epsilon_polynomial_decay_step\n",
    "        self.target_soft_update = target_soft_update\n",
    "        self.target_update_step = target_update_step\n",
    "        self.save_folder = '.model'\n",
    "        self.save_term = save_term\n",
    "        self.evaluation_term = evaluation_term\n",
    "        self.show_phase = show_phase\n",
    "\n",
    "config = Config({0:[0], 1:[1], 2:[2], 3: [3], 4:[4]}, [0, 1], 120000, '', 0.1, 0.001, 32, [0.01, 1], 120000, 1., 1000, 120000, 1000, 'Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f0986dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(config.new_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e9a7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.x_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e95127c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17117\n",
      "4280\n",
      "17117\n",
      "4280\n",
      "No of total samples in dataset and their distribution:  (array([0, 1, 2, 3, 4], dtype=int64), array([ 1087,  2189,  2386, 13158,  2577], dtype=int64))\n",
      "\n",
      "Reward for each class.\n",
      "\t- Class 0 : 0.781136\n",
      "\t- Class 1 : 0.387891\n",
      "\t- Class 2 : 0.355865\n",
      "\t- Class 3 : 0.064531\n",
      "\t- Class 4 : 0.329489\n",
      "Label is  [0 1 2 3 4]\n",
      "Labels with count {0: 1087, 1: 2189, 2: 2386, 3: 13158, 4: 2577}\n"
     ]
    }
   ],
   "source": [
    "dataset = CassavaLeafDataset(image_size = (128, 128), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542419c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dbe3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:63: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  x = tf.compat.v1.layers.conv2d(self.state, 64, 5, strides=2, activation=tf.nn.relu)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:64: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  x = tf.compat.v1.layers.conv2d(x, 32, 5, strides=2, activation=tf.nn.relu)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:65: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
      "  x = tf.compat.v1.layers.max_pooling2d(x, (2,2), strides=2)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:67: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  x = tf.compat.v1.layers.conv2d(x, 64, 5, strides=1, activation=tf.nn.relu)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:68: UserWarning: `tf.layers.conv2d` is deprecated and will be removed in a future version. Please Use `tf.keras.layers.Conv2D` instead.\n",
      "  x = tf.compat.v1.layers.conv2d(x, 32, 5, strides=1, activation=tf.nn.relu)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:69: UserWarning: `tf.layers.max_pooling2d` is deprecated and will be removed in a future version. Please use `tf.keras.layers.MaxPooling2D` instead.\n",
      "  x = tf.compat.v1.layers.max_pooling2d(x, (2,2), strides=2)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:71: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  x = tf.compat.v1.layers.flatten(x)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:72: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  x = tf.compat.v1.layers.dense(x, 256, activation=tf.nn.relu)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:73: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  x = tf.compat.v1.layers.dense(x, 128, activation=tf.nn.relu)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:74: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  a = tf.compat.v1.layers.dense(x, self.n_class)\n",
      "C:\\Users\\hassa\\OneDrive\\Desktop\\AI Degree\\Knowledge Representation and Reasonins\\DRL-on-Imbalanced-Data\\network.py:75: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
      "  v = tf.compat.v1.layers.dense(x, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    }
   ],
   "source": [
    "q_network = QNetwork(config, 128, 'complex')\n",
    "memory = Memory()\n",
    "agent = Agent(q_network, dataset, memory, config)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ea123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
